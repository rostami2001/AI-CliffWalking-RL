{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "449ca2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 0.]\n",
      "[ 0.  0. -1. -1.  0.]\n",
      "[0. 0. 0. 0. 0.]\n",
      "[ 0. -1. -1.  0.  0.]\n",
      "[0. 0. 0. 0. 0.]\n",
      "Episode: 0\n",
      "State    Up     Right    Down    Left\n",
      "[0, 0] ['0.00', '0.00', '0.00', '0.00']\n",
      "[0, 1] ['0.00', '0.00', '0.00', '0.00']\n",
      "[0, 2] ['0.00', '0.00', '0.00', '0.00']\n",
      "[0, 3] ['0.00', '0.00', '-0.90', '0.00']\n",
      "[0, 4] ['0.00', '0.00', '0.00', '0.00']\n",
      "[1, 0] ['0.00', '0.00', '0.00', '0.00']\n",
      "[1, 1] ['0.00', '0.00', '0.00', '0.00']\n",
      "[1, 2] ['0.00', '0.00', '0.00', '0.00']\n",
      "[1, 3] ['0.00', '0.00', '0.00', '0.00']\n",
      "[1, 4] ['0.00', '0.00', '0.00', '0.00']\n",
      "[2, 0] ['0.00', '0.00', '0.00', '0.00']\n",
      "[2, 1] ['0.00', '0.00', '0.00', '0.00']\n",
      "[2, 2] ['0.00', '0.00', '0.00', '0.00']\n",
      "[2, 3] ['0.00', '0.00', '0.00', '0.00']\n",
      "[2, 4] ['0.00', '0.00', '0.00', '0.00']\n",
      "[3, 0] ['0.00', '0.00', '0.00', '0.00']\n",
      "[3, 1] ['0.00', '0.00', '0.00', '0.00']\n",
      "[3, 2] ['0.00', '0.00', '0.00', '0.00']\n",
      "[3, 3] ['0.00', '0.00', '0.00', '0.00']\n",
      "[3, 4] ['0.00', '0.00', '0.00', '0.00']\n",
      "[4, 0] ['0.00', '0.00', '0.00', '0.00']\n",
      "[4, 1] ['0.00', '0.00', '0.00', '0.00']\n",
      "[4, 2] ['0.00', '0.00', '0.00', '0.00']\n",
      "[4, 3] ['0.00', '0.00', '0.00', '0.00']\n",
      "[4, 4] ['0.00', '0.00', '0.00', '0.00']\n",
      "Episode: 1\n",
      "State    Up     Right    Down    Left\n",
      "[0, 0] ['0.00', '0.00', '0.00', '0.00']\n",
      "[0, 1] ['0.00', '0.90', '0.00', '0.00']\n",
      "[0, 2] ['0.00', '0.00', '0.00', '0.00']\n",
      "[0, 3] ['0.00', '0.00', '-0.90', '0.00']\n",
      "[0, 4] ['0.00', '0.00', '0.00', '0.00']\n",
      "[1, 0] ['0.00', '0.00', '0.00', '0.00']\n",
      "[1, 1] ['0.00', '0.00', '0.00', '0.00']\n",
      "[1, 2] ['0.00', '0.00', '0.00', '0.00']\n",
      "[1, 3] ['0.00', '0.00', '0.00', '0.00']\n",
      "[1, 4] ['0.00', '0.00', '0.00', '0.00']\n",
      "[2, 0] ['0.00', '0.00', '0.00', '0.00']\n",
      "[2, 1] ['0.00', '0.00', '0.00', '0.00']\n",
      "[2, 2] ['0.00', '0.00', '0.00', '0.00']\n",
      "[2, 3] ['0.00', '0.00', '0.00', '0.00']\n",
      "[2, 4] ['0.00', '0.00', '0.00', '0.00']\n",
      "[3, 0] ['0.00', '0.00', '0.00', '0.00']\n",
      "[3, 1] ['0.00', '0.00', '0.00', '0.00']\n",
      "[3, 2] ['0.00', '0.00', '0.00', '0.00']\n",
      "[3, 3] ['0.00', '0.00', '0.00', '0.00']\n",
      "[3, 4] ['0.00', '0.00', '0.00', '0.00']\n",
      "[4, 0] ['0.00', '0.00', '0.00', '0.00']\n",
      "[4, 1] ['0.00', '0.00', '0.00', '0.00']\n",
      "[4, 2] ['0.00', '0.00', '0.00', '0.00']\n",
      "[4, 3] ['0.00', '0.00', '0.00', '0.00']\n",
      "[4, 4] ['0.00', '0.00', '0.00', '0.00']\n",
      "Episode: 10\n",
      "State    Up     Right    Down    Left\n",
      "[0, 0] ['0.00', '0.90', '0.00', '0.59']\n",
      "[0, 1] ['0.00', '1.00', '0.00', '0.00']\n",
      "[0, 2] ['0.00', '0.00', '0.00', '0.00']\n",
      "[0, 3] ['0.00', '0.00', '-0.90', '0.00']\n",
      "[0, 4] ['0.00', '0.00', '0.00', '0.00']\n",
      "[1, 0] ['0.77', '0.00', '0.00', '0.00']\n",
      "[1, 1] ['0.89', '0.00', '0.00', '0.00']\n",
      "[1, 2] ['0.00', '0.00', '0.00', '0.00']\n",
      "[1, 3] ['0.00', '0.00', '0.00', '0.00']\n",
      "[1, 4] ['0.00', '0.00', '0.00', '-0.90']\n",
      "[2, 0] ['0.48', '0.00', '0.00', '0.00']\n",
      "[2, 1] ['0.00', '0.00', '0.00', '0.00']\n",
      "[2, 2] ['-0.90', '0.00', '0.00', '0.00']\n",
      "[2, 3] ['-0.90', '0.00', '0.00', '0.00']\n",
      "[2, 4] ['0.00', '0.00', '0.00', '0.00']\n",
      "[3, 0] ['0.00', '0.00', '0.00', '0.00']\n",
      "[3, 1] ['0.00', '0.00', '0.00', '0.00']\n",
      "[3, 2] ['0.00', '0.00', '0.00', '0.00']\n",
      "[3, 3] ['0.00', '0.00', '0.00', '0.00']\n",
      "[3, 4] ['0.00', '0.00', '0.00', '0.00']\n",
      "[4, 0] ['0.00', '0.00', '0.00', '0.00']\n",
      "[4, 1] ['0.00', '0.00', '0.00', '0.00']\n",
      "[4, 2] ['-0.90', '0.00', '0.00', '0.00']\n",
      "[4, 3] ['0.00', '0.00', '0.00', '0.00']\n",
      "[4, 4] ['0.00', '0.00', '0.00', '0.00']\n",
      "Episode: 99\n",
      "State    Up     Right    Down    Left\n",
      "[0, 0] ['0.73', '0.90', '0.73', '0.81']\n",
      "[0, 1] ['0.89', '1.00', '0.73', '0.00']\n",
      "[0, 2] ['0.00', '0.00', '0.00', '0.00']\n",
      "[0, 3] ['0.90', '0.00', '-0.90', '1.00']\n",
      "[0, 4] ['0.73', '0.80', '0.72', '0.90']\n",
      "[1, 0] ['0.81', '0.81', '0.65', '0.00']\n",
      "[1, 1] ['0.90', '-0.90', '0.00', '0.00']\n",
      "[1, 2] ['0.00', '0.00', '0.00', '0.00']\n",
      "[1, 3] ['0.00', '0.00', '0.00', '0.00']\n",
      "[1, 4] ['0.81', '0.73', '0.63', '-1.00']\n",
      "[2, 0] ['0.73', '0.73', '0.00', '0.00']\n",
      "[2, 1] ['0.81', '0.00', '0.00', '0.65']\n",
      "[2, 2] ['-0.90', '0.59', '0.00', '0.00']\n",
      "[2, 3] ['-1.00', '0.66', '0.48', '0.00']\n",
      "[2, 4] ['0.73', '0.65', '0.53', '0.59']\n",
      "[3, 0] ['0.66', '-0.90', '0.00', '0.00']\n",
      "[3, 1] ['0.00', '0.00', '0.00', '0.00']\n",
      "[3, 2] ['0.00', '0.00', '0.00', '0.00']\n",
      "[3, 3] ['0.59', '0.00', '0.00', '-0.99']\n",
      "[3, 4] ['0.66', '0.00', '0.00', '0.00']\n",
      "[4, 0] ['0.59', '0.00', '0.48', '0.00']\n",
      "[4, 1] ['-0.90', '0.42', '0.00', '0.00']\n",
      "[4, 2] ['-0.90', '0.48', '0.00', '0.00']\n",
      "[4, 3] ['0.53', '0.00', '0.00', '0.00']\n",
      "[4, 4] ['0.47', '0.00', '0.00', '0.00']\n",
      "Training complete!\n",
      "[[4, 2], [4, 3], [3, 3], [2, 3], [2, 4], [1, 4], [0, 4], [0, 3], [0, 2]]\n",
      "[[3, 4], [2, 4], [1, 4], [0, 4], [0, 3], [0, 2]]\n",
      "[[3, 0], [2, 0], [1, 0], [0, 0], [0, 1], [0, 2]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Q-Learning Assign_1.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1J_u-LplrhKpc1DG_nbmNSxA-LLTetDqK\n",
    "\n",
    "## Scenario - Robot in a 5X5 grid\n",
    "In a 5X5 grid environment, robots search for the path to reach the charging dock area from given position i.e. [4,2] or wherever it is in 10 steps.\n",
    "\n",
    "In order to ensure maximum efficiency the robots will need to learn the shortest path between the chargin dock area and all other locations within the grid where the robots are allowed to travel.\n",
    "* We will use Q-learning to accomplish this task!\n",
    "\"\"\"\n",
    "\n",
    "#import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"## Define the Environment\n",
    "The environment consists of **states**, **actions**, and **rewards**. States and actions are inputs for the Q-learning AI agent, while the possible actions are the AI agent's outputs.\n",
    "#### States\n",
    "The states in the environment are all of the possible locations within the grid. Some of these locations are inaccessible (**gray squares**), while other locations are aisles that the robot can use to travel throughout the grid (**white squares**). The **charging dock area is in yellow cell** indicates the charging dock area.\n",
    "\n",
    "The gray squares are **terminal states**!\n",
    "\n",
    "![picture](https://drive.google.com/uc?export=view&id=1Jdu7axfpNn0EzV2oiENcIxo7ghiLEjf9)\n",
    "\n",
    "The AI agent's goal is to learn the shortest path between the charging dock area and all of the other locations in the grid where the robot is allowed to travel.\n",
    "\n",
    "As shown in the image above, there are 25 possible states (locations) within the grid. These states are arranged in a grid containing 5 rows and 5 columns. Each location can hence be identified by its row and column index.\n",
    "\"\"\"\n",
    "\n",
    "#define the shape of the environment (i.e., its states)\n",
    "environment_rows = 5\n",
    "environment_columns = 5\n",
    "\n",
    "#Create a 3D numpy array to hold the current Q-values for each state and action pair: Q(s, a) \n",
    "#The array contains 5 rows and 5 columns (to match the shape of the environment), as well as a third \"action\" dimension.\n",
    "#The \"action\" dimension consists of 4 layers that will allow us to keep track of the Q-values for each possible action in\n",
    "#each state (see next cell for a description of possible actions). \n",
    "#The value of each (state, action) pair is initialized to 0.\n",
    "q_values = np.zeros((environment_rows, environment_columns, 4))\n",
    "\n",
    "\"\"\"#### Actions\n",
    "The actions that are available to the AI agent are to move the robot in one of four directions:\n",
    "* Up\n",
    "* Right\n",
    "* Down\n",
    "* Left\n",
    "\n",
    "Obviously, the AI agent must learn to avoid driving into the inaccessible locations!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#define actions\n",
    "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "actions = ['up', 'right', 'down', 'left']\n",
    "\n",
    "\"\"\"#### Rewards\n",
    "The last component of the environment that we need to define are the **rewards**. \n",
    "\n",
    "To help the AI agent learn, each state (location) in the grid is assigned a reward value.\n",
    "\n",
    "The agent may begin at any white square, but its goal is always the same: ***to maximize its total rewards***!\n",
    "\n",
    "To maximize its cumulative rewards , the AI agent will need find the shortest paths between the charging dock area (yellow square) and all of the other locations in the grid where the robot is allowed to travel (white squares). The agent will also need to learn to avoid crashing into any of the inaccessible areas (black squares)!\n",
    "\"\"\"\n",
    "\n",
    "#Create a 2D numpy array to hold the rewards for each state. \n",
    "#The array contains 5 rows and 5 columns (to match the shape of the environment), and each value is initialized to 0.\n",
    "rewards = np.full((environment_rows, environment_columns), 0.)\n",
    "rewards[0, 2] = 1. #set the reward for the charging dock area (i.e., the goal) to 1\n",
    "\n",
    "#define inaccessible locations (i.e., black squares) for rows 1 through 5\n",
    "inacc = {} #inaccessible locations in a dictionary\n",
    "inacc[1] = [2, 3]\n",
    "inacc[3] = [1, 2]\n",
    "\n",
    "#set the rewards for all inaccessible locations (i.e., black squares)\n",
    "for row_index in [1,3]:\n",
    "  for column_index in inacc[row_index]:\n",
    "    rewards[row_index, column_index] = -1\n",
    "  \n",
    "# print rewards matrix\n",
    "for row in rewards:\n",
    "  print(row)\n",
    "\n",
    "\"\"\"## Train the Model\n",
    "Our next task is for our AI agent to learn about its environment by implementing a Q-learning model. The learning process will follow these steps:\n",
    "1. Choose a random, non-terminal state (white square) for the agent to begin this new episode.\n",
    "2. Choose an action (move *up*, *right*, *down*, or *left*) for the current state. Actions will be chosen using an *epsilon greedy algorithm*. This algorithm will usually choose the most promising action for the AI agent, but it will occasionally choose a less promising option in order to encourage the agent to explore the environment.\n",
    "3. Perform the chosen action, and transition to the next state (i.e., move to the next location).\n",
    "4. Receive the reward for moving to the new state, and calculate the temporal difference.\n",
    "5. Update the Q-value for the previous state and action pair.\n",
    "6. If the new (current) state is a terminal state, go to #1. Else, go to #2.\n",
    "\n",
    "This entire process will be repeated across 1000 episodes. This will provide the AI agent sufficient opportunity to learn the shortest paths between the charging dock area and all other locations in the grid where the robot is allowed to travel, while simultaneously avoiding crashing into any of the inaccessible locations!\n",
    "\n",
    "#### Define Helper Functions\n",
    "\"\"\"\n",
    "\n",
    "#define a function that determines if the specified location is a terminal state\n",
    "def is_terminal_state(current_row_index, current_column_index):\n",
    "  #if the reward for this location is 0, then it is not a terminal state (i.e., it is a 'white square')\n",
    "  if rewards[current_row_index, current_column_index] == 0.:\n",
    "    return False\n",
    "  else:\n",
    "    return True\n",
    "\n",
    "#define a function that will choose a random, non-terminal starting location\n",
    "def get_starting_location():\n",
    "  #get a random row and column index\n",
    "  current_row_index = np.random.randint(environment_rows)\n",
    "  current_column_index = np.random.randint(environment_columns)\n",
    "  #continue choosing random row and column indexes until a non-terminal state is identified\n",
    "  #(i.e., until the chosen state is a 'white square').\n",
    "  while is_terminal_state(current_row_index, current_column_index):\n",
    "    current_row_index = np.random.randint(environment_rows)\n",
    "    current_column_index = np.random.randint(environment_columns)\n",
    "  return current_row_index, current_column_index\n",
    "\n",
    "#define an epsilon greedy algorithm that will choose which action to take next (i.e., where to move next)\n",
    "def get_next_action(current_row_index, current_column_index, epsilon):\n",
    "  #if a randomly chosen value between 0 and 1 is less than epsilon, \n",
    "  #then choose the most promising value from the Q-table for this state.\n",
    "  if np.random.random() < epsilon:\n",
    "    return np.argmax(q_values[current_row_index, current_column_index])\n",
    "  else: #choose a random action\n",
    "    return np.random.randint(4)\n",
    "\n",
    "#define a function that will get the next location based on the chosen action\n",
    "def get_next_location(current_row_index, current_column_index, action_index):\n",
    "  new_row_index = current_row_index\n",
    "  new_column_index = current_column_index\n",
    "  if actions[action_index] == 'up' and current_row_index > 0:\n",
    "    new_row_index -= 1\n",
    "  elif actions[action_index] == 'right' and current_column_index < environment_columns - 1:\n",
    "    new_column_index += 1\n",
    "  elif actions[action_index] == 'down' and current_row_index < environment_rows - 1:\n",
    "    new_row_index += 1\n",
    "  elif actions[action_index] == 'left' and current_column_index > 0:\n",
    "    new_column_index -= 1\n",
    "  return new_row_index, new_column_index\n",
    "\n",
    "#define a function that will get the shortest path between any location within the grid that \n",
    "#the robot is allowed to travel and the charging dock location.\n",
    "def get_shortest_path(start_row_index, start_column_index):\n",
    "  #return immediately if this is an invalid starting location\n",
    "  if is_terminal_state(start_row_index, start_column_index):\n",
    "    return []\n",
    "  else: #if this is a 'legal' starting location\n",
    "    current_row_index, current_column_index = start_row_index, start_column_index\n",
    "    shortest_path = []\n",
    "    shortest_path.append([current_row_index, current_column_index])\n",
    "    #continue moving along the path until we reach the goal (i.e., the charging dock location)\n",
    "    while not is_terminal_state(current_row_index, current_column_index):\n",
    "      #get the best action to take\n",
    "      action_index = get_next_action(current_row_index, current_column_index, 1.)\n",
    "      #move to the next location on the path, and add the new location to the list\n",
    "      current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n",
    "      shortest_path.append([current_row_index, current_column_index])\n",
    "    return shortest_path\n",
    "\n",
    "\"\"\"#### Train the AI Agent using Q-Learning\"\"\"\n",
    "\n",
    "#define training parameters\n",
    "epsilon = 0.8 #the percentage of time when we should take the best action (instead of a random action)\n",
    "discount_factor = 0.9 #discount factor for future rewards\n",
    "learning_rate = 0.9 #the rate at which the AI agent should learn\n",
    "episodes = 100 #number of training episodes\n",
    "\n",
    "#initialize reward for each location for each episode to zero\n",
    "rewards_x = np.zeros(episodes)\n",
    "\n",
    "#run through 100 training episodes\n",
    "for episode in range(episodes):\n",
    "  #get the starting location for this episode\n",
    "  row_index, column_index = get_starting_location()\n",
    "  #row_index, column_index = 4, 2\n",
    "\n",
    "  #continue taking actions (i.e., moving) until we reach a terminal state\n",
    "  #(i.e., until we reach the charging dock area or crash into an inaccessible location)\n",
    "  while not is_terminal_state(row_index, column_index):\n",
    "    #choose which action to take (i.e., where to move next)\n",
    "    action_index = get_next_action(row_index, column_index, epsilon)\n",
    "\n",
    "    #perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
    "    old_row_index, old_column_index = row_index, column_index #store the old row and column indexes\n",
    "    row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
    "    \n",
    "    #receive the reward for moving to the new state, and calculate the temporal difference\n",
    "    reward = rewards[row_index, column_index]\n",
    "    old_q_value = q_values[old_row_index, old_column_index, action_index]\n",
    "    temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n",
    "\n",
    "    #update rewards of each episode for each location\n",
    "    rewards_x[episode] = reward\n",
    "\n",
    "    #update the Q-value for the previous state and action pair\n",
    "    new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "    q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "\n",
    "  if episode == 0 or (episode == 1 or (episode == 10 or episode == episodes-1)):\n",
    "    print('Episode: %d' % episode)\n",
    "    print('State    Up     Right    Down    Left')\n",
    "    for i in range(5):\n",
    "      for j in range(5):\n",
    "        formatted_q_values = ['%.2f' % member for member in q_values[i,j]]\n",
    "        #print(q_values[4,2])\n",
    "        print('[%d, %d]' %(i,j), formatted_q_values)\n",
    "\n",
    "print('Training complete!')\n",
    "rewards_tot = np.cumsum(rewards_x)\n",
    "plt.bar(range(episodes),rewards_tot)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Total Reward')\n",
    "\n",
    "\"\"\"## Get Shortest Paths\n",
    "Now that the AI agent has been fully trained, we can see what it has learned by displaying the shortest path between any location in the grid where the robot is allowed to travel and the charging dock area.\n",
    "\n",
    "Run the code cell below to try a few different starting locations!\n",
    "\"\"\"\n",
    "\n",
    "#display a few shortest paths\n",
    "print(get_shortest_path(4, 2)) #starting at row 9, column 5\n",
    "print(get_shortest_path(3, 4)) #starting at row 3, column 9\n",
    "print(get_shortest_path(3, 0)) #starting at row 5, column 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eabeaf4",
   "metadata": {},
   "source": [
    "# SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d105232f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 2], [4, 3], [3, 3], [3, 4], [2, 4], [1, 4], [0, 4], [0, 3], [0, 2]]\n",
      "[[3, 4], [2, 4], [1, 4], [0, 4], [0, 3], [0, 2]]\n",
      "[[3, 0], [2, 0], [1, 0], [0, 0], [0, 1], [0, 2]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the environment\n",
    "environment_rows = 5\n",
    "environment_columns = 5\n",
    "\n",
    "# Initialize Q-values\n",
    "q_values = np.zeros((environment_rows, environment_columns, 4))\n",
    "\n",
    "# Define actions\n",
    "actions = ['up', 'right', 'down', 'left']\n",
    "\n",
    "# Initialize rewards\n",
    "rewards = np.full((environment_rows, environment_columns), 0.)\n",
    "rewards[0, 2] = 1.  # Charging dock area reward\n",
    "\n",
    "# Define inaccessible locations\n",
    "inacc = {1: [2, 3], 3: [1, 2]}\n",
    "for row_index in [1, 3]:\n",
    "    for column_index in inacc[row_index]:\n",
    "        rewards[row_index, column_index] = -1\n",
    "\n",
    "# Helper functions\n",
    "def is_terminal_state(current_row_index, current_column_index):\n",
    "    return rewards[current_row_index, current_column_index] != 0.\n",
    "\n",
    "def get_starting_location():\n",
    "    current_row_index = np.random.randint(environment_rows)\n",
    "    current_column_index = np.random.randint(environment_columns)\n",
    "    while is_terminal_state(current_row_index, current_column_index):\n",
    "        current_row_index = np.random.randint(environment_rows)\n",
    "        current_column_index = np.random.randint(environment_columns)\n",
    "    return current_row_index, current_column_index\n",
    "\n",
    "def get_next_action(current_row_index, current_column_index, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.argmax(q_values[current_row_index, current_column_index])\n",
    "    else:\n",
    "        return np.random.randint(4)\n",
    "\n",
    "def get_next_location(current_row_index, current_column_index, action_index):\n",
    "    new_row_index = current_row_index\n",
    "    new_column_index = current_column_index\n",
    "    if actions[action_index] == 'up' and current_row_index > 0:\n",
    "        new_row_index -= 1\n",
    "    elif actions[action_index] == 'right' and current_column_index < environment_columns - 1:\n",
    "        new_column_index += 1\n",
    "    elif actions[action_index] == 'down' and current_row_index < environment_rows - 1:\n",
    "        new_row_index += 1\n",
    "    elif actions[action_index] == 'left' and current_column_index > 0:\n",
    "        new_column_index -= 1\n",
    "    return new_row_index, new_column_index\n",
    "\n",
    "def get_shortest_path(start_row_index, start_column_index):\n",
    "    if is_terminal_state(start_row_index, start_column_index):\n",
    "        return []\n",
    "    else:\n",
    "        current_row_index, current_column_index = start_row_index, start_column_index\n",
    "        shortest_path = []\n",
    "        shortest_path.append([current_row_index, current_column_index])\n",
    "        while not is_terminal_state(current_row_index, current_column_index):\n",
    "            action_index = get_next_action(current_row_index, current_column_index, 1.)\n",
    "            current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n",
    "            shortest_path.append([current_row_index, current_column_index])\n",
    "        return shortest_path\n",
    "\n",
    "# SARSA parameters\n",
    "epsilon = 0.8\n",
    "discount_factor = 0.9\n",
    "learning_rate = 0.9\n",
    "episodes = 100\n",
    "\n",
    "# Initialize rewards for each episode\n",
    "rewards_x = np.zeros(episodes)\n",
    "\n",
    "# SARSA training\n",
    "for episode in range(episodes):\n",
    "    row_index, column_index = get_starting_location()\n",
    "    action_index = get_next_action(row_index, column_index, epsilon)\n",
    "\n",
    "    while not is_terminal_state(row_index, column_index):\n",
    "        old_row_index, old_column_index = row_index, column_index\n",
    "        old_action_index = action_index\n",
    "        \n",
    "        row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
    "        reward = rewards[row_index, column_index]\n",
    "        rewards_x[episode] += reward\n",
    "        \n",
    "        next_action_index = get_next_action(row_index, column_index, epsilon)\n",
    "\n",
    "        old_q_value = q_values[old_row_index, old_column_index, old_action_index]\n",
    "        next_q_value = q_values[row_index, column_index, next_action_index]\n",
    "        temporal_difference = reward + (discount_factor * next_q_value) - old_q_value\n",
    "        \n",
    "        new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "        q_values[old_row_index, old_column_index, old_action_index] = new_q_value\n",
    "\n",
    "        action_index = next_action_index\n",
    "\n",
    "# Plotting the total rewards\n",
    "rewards_tot = np.cumsum(rewards_x)\n",
    "plt.bar(range(episodes), rewards_tot)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.show()\n",
    "\n",
    "# Display a few shortest paths\n",
    "print(get_shortest_path(4, 2))\n",
    "print(get_shortest_path(3, 4))\n",
    "print(get_shortest_path(3, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb51f678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728f4ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
